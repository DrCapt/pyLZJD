{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was adapted from https://www.cs.jhu.edu/~mdredze/datasets/image_spam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyLZJD import hash, sim, vectorize\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import urllib.parse\n",
    "import glob\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_url = \"https://www.cs.jhu.edu/~mdredze/datasets/image_spam/personal_image_spam.tar.gz\"\n",
    "ham_url = \"https://www.cs.jhu.edu/~mdredze/datasets/image_spam/personal_image_ham.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"personal_image_spam\"):\n",
    "    print(\"Downloading dataset\")\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "\n",
    "    urllib.request.urlretrieve(ham_url, 'personal_image_ham.tar.gz')\n",
    "    urllib.request.urlretrieve(spam_url, 'personal_image_spam.tar.gz')\n",
    "    \n",
    "    tf = tarfile.open(\"personal_image_ham.tar.gz\")\n",
    "    tf.extractall()\n",
    "    tf = tarfile.open(\"personal_image_spam.tar.gz\")\n",
    "    tf.extractall()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_paths = glob.glob(\"personal_image_spam/*\")\n",
    "ham_paths = glob.glob(\"personal_image_ham/*\")\n",
    "\n",
    "\n",
    "all_paths = spam_paths + ham_paths\n",
    "yBad = [1 for i in range(len(spam_paths))]\n",
    "yGood = [0 for i in range(len(ham_paths))]\n",
    "y = yBad + yGood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorize(all_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #splitting data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train) #training our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.946429\n",
      "Precision: 0.950437\n",
      "Recall: 0.965926\n",
      "F1-Score: 0.958119\n",
      "AUC: 0.987108\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Evaluation #\n",
    "##############\n",
    "\n",
    "predicted = lgs.predict(X_test)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, (lgs.predict_proba(X_test)[:, 1]))\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"Accuracy: %f\" % lgs.score(X_test, y_test))  #checking the accuracy\n",
    "print(\"Precision: %f\" % metrics.precision_score(y_test, predicted))\n",
    "print(\"Recall: %f\" % metrics.recall_score(y_test, predicted))\n",
    "print(\"F1-Score: %f\" % metrics.f1_score(y_test, predicted))\n",
    "print(\"AUC: %f\" % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LZJD's 'false seen probability', we can sometimes improve the accuracy and robustness of our models by over-sampling the training data. For most ML techniques, this has no benefit. LZJD it works because on each sampling, LZJD will produce a \"different but valid\" representation for the same file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_train, paths_test, y_train, y_test = train_test_split(all_paths, y, test_size=0.2, random_state=42) #splitting data\n",
    "\n",
    "X_train = vectorize(paths_train*10, false_seen_prob=0.05)\n",
    "X_test = vectorize(paths_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train*10) #training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956767\n",
      "Precision: 0.953824\n",
      "Recall: 0.979259\n",
      "F1-Score: 0.966374\n",
      "AUC: 0.991602\n"
     ]
    }
   ],
   "source": [
    "predicted = lgs.predict(X_test)\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, (lgs.predict_proba(X_test)[:, 1]))\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"Accuracy: %f\" % lgs.score(X_test, y_test))  #checking the accuracy\n",
    "print(\"Precision: %f\" % metrics.precision_score(y_test, predicted))\n",
    "print(\"Recall: %f\" % metrics.recall_score(y_test, predicted))\n",
    "print(\"F1-Score: %f\" % metrics.f1_score(y_test, predicted))\n",
    "print(\"AUC: %f\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
